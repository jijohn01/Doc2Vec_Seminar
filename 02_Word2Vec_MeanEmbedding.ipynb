{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrXedYuu3qaypG9z89xmnT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Library 설치"],"metadata":{"id":"0_R6m5q4Iafv"}},{"cell_type":"markdown","source":["## 1_Model 다운로드"],"metadata":{"id":"KNStXwr3IkjP"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# 구글의 Word2Vec 사전 학습 모델 Colab에서 연결 Error 발생 -> Local에서 하면 문제 없음\n","model = api.load('word2vec-google-news-300')"],"metadata":{"id":"hPy8p1ZIIjRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2_Word2Vec을 활용한 Document Embedding"],"metadata":{"id":"702YV0ACIrNV"}},{"cell_type":"code","source":["import re\n","from gensim.utils import simple_preprocess\n","\n","def get_word_vectors(sentence, model):\n","    words = simple_preprocess(sentence)\n","    words = [word for word in words if word in model.index_to_key]\n","    vectors = []\n","    for word in words:\n","        if word in model.index_to_key:\n","            vectors.append(model[word])\n","    return vectors"],"metadata":{"id":"TbOmCuhWIqWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_document_embedding(sentence, model):\n","    vectors = get_word_vectors(sentence, model)\n","    if not vectors:\n","        return None\n","    document_embedding = np.mean(vectors, axis=0)\n","    return document_embedding\n"],"metadata":{"id":"N-wkgg2JIu_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = 'The cat is sleeping on the bed.'\n","document_embedding = get_document_embedding(sentence, model)\n","\n","words = simple_preprocess(sentence)\n","words = [word for word in words if word in model.index_to_key]"],"metadata":{"id":"A16Ff3RXIwHk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 시각화"],"metadata":{"id":"naiswbudI6d4"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt \n","from sklearn.decomposition import PCA\n","\n","\n","ref_words = [\"Sentence\"] + words # 시각화할 단어와 유사한 단어들\n","vectors = [document_embedding] + [model[word] for word in ref_words[1:]]  # 시각화할 단어와 유사한 단어들의 벡터값\n","\n","pca = PCA(n_components=2)\n","vectors_2d = pca.fit_transform(vectors)\n","\n","\n","fig, ax = plt.subplots()\n","for word, vector in zip(ref_words, vectors_2d):\n","    ax.annotate(word, vector)  # 단어 이름을 벡터값 위에 표시\n","ax.scatter(vectors_2d[:, 0], vectors_2d[:, 1])  # 벡터값으로 산점도 그리기\n","\n","plt.scatter(vectors_2d[0, 0], vectors_2d[0, 1], color='red')\n","plt.show()"],"metadata":{"id":"5JuVSLHgI4BD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3_쿼리와 Mean Word Embedding"],"metadata":{"id":"z-9G9kZUJFUb"}},{"cell_type":"code","source":["docs = ['The cat is sitting on the mat.',\n","        'The dog is lying on the rug.',\n","        'I love pizza and spaghetti for dinner.',\n","        'I like to drink coffee in the morning.']\n","\n","docs_embed =[get_document_embedding(sentence, model) for sentence in docs]\n","\n","qry = 'The cat is sleeping on the bed.'\n","qry_embed = [get_document_embedding(qry, model)]\n","\n","ref_words = [qry] + docs # 시각화할 단어와 유사한 단어들\n","vectors = qry_embed + docs_embed # 시각화할 단어와 유사한 단어들의 벡터값"],"metadata":{"id":"rJer79fGJEfr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 시각화"],"metadata":{"id":"c7zrwMspJNXH"}},{"cell_type":"code","source":["pca = PCA(n_components=2)\n","vectors_2d = pca.fit_transform(vectors)\n","\n","fig, ax = plt.subplots()\n","for word, vector in zip(ref_words, vectors_2d):\n","    ax.annotate(word, vector)  # 단어 이름을 벡터값 위에 표시\n","ax.scatter(vectors_2d[:, 0], vectors_2d[:, 1])  # 벡터값으로 산점도 그리기\n","\n","plt.scatter(vectors_2d[0, 0], vectors_2d[0, 1], color='red')\n","plt.show()"],"metadata":{"id":"ipVkaestJNCB"},"execution_count":null,"outputs":[]}]}